{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello MP1!\n"
     ]
    }
   ],
   "source": [
    "print('Hello MP1!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imported libraries used for the project\n",
    "1. jupiter\n",
    "2. scikit-learn\n",
    "3. gensim\n",
    "4. nltk\n",
    "5. numpy\n",
    "6. pandas\n",
    "7. matplotlib\n",
    "\n",
    "`conda install jupiter scikit-learn gensim nltk numpy pandas matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation & Analysis (5pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Load the dataset. You can use `gzip.open` and `json.load` to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "dataset = gzip.open('goemotions.json.gz')\n",
    "dataset_json = json.load(dataset)\n",
    "\n",
    "# Close the gz dataset once your finished loading the data as a json object\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. (5pts) Extract the posts and the 2 sets of labels (emotion and sentiment), then plot the distribution\n",
    "of the posts in each category and save the graphic (a histogram or pie chart) in pdf. Do this for both\n",
    "the emotion and the sentiment categories. You can use `matplotlib.pyplot` and `savefig` to do this.\n",
    "This pre-analysis of the dataset will allow you to determine if the classes are balanced, and which\n",
    "metric is more appropriate to use to evaluate the performance of your classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "numpy_dataset = np.array(dataset_json)\n",
    "\n",
    "# Get column only for emotion and sentiment\n",
    "emotion_dataset_col = numpy_dataset[:, 1]\n",
    "sentiment_dataset_col = numpy_dataset[:, 2]\n",
    "\n",
    "# Count the number of times each value appears\n",
    "emotion_count = Counter(emotion_dataset_col)\n",
    "sentiment_count = Counter(sentiment_dataset_col)\n",
    "\n",
    "# Save the data values as a histogram\n",
    "plt.hist(emotion_count.values())\n",
    "plt.savefig('emotions_graph')\n",
    "\n",
    "plt.close()\n",
    "\n",
    "\n",
    "plt.hist(sentiment_count.values())\n",
    "plt.savefig('sentiment_graph')\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Words as Features (35pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. □ (5pts) Process the dataset using `feature_extraction.text.CountVectorizer` to extract tokens/words\n",
    "and their frequencies. Display the number of tokens (the size of the vocabulary) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (tokens in the vocabulary) = 30449\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import pandas as pd\n",
    "\n",
    "\n",
    "# Phrases are in the first column of the dataset\n",
    "phrases = numpy_dataset[:, 0]\n",
    "\n",
    "# Process the dataset\n",
    "vectorizer_emotions = CountVectorizer()\n",
    "\n",
    "# X value is the processed_dataset\n",
    "X_emotions = vectorizer_emotions.fit_transform(phrases)\n",
    "\n",
    "print(\"Number of features (tokens in the vocabulary) =\",\n",
    "      len(vectorizer_emotions.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (tokens in the vocabulary) including emotions = 30450\n"
     ]
    }
   ],
   "source": [
    "emotions = numpy_dataset[:, 1]\n",
    "\n",
    "emotions_and_phrases = phrases.copy()\n",
    "\n",
    "for count, i in enumerate(phrases):\n",
    "    emotions_and_phrases[count] = i + \" \" + emotions[count]\n",
    "\n",
    "\n",
    "vectorizer_sentiments = CountVectorizer()\n",
    "X_sentiments = vectorizer_sentiments.fit_transform(emotions_and_phrases)\n",
    "\n",
    "print(\"Number of features (tokens in the vocabulary) including emotions =\",\n",
    "      len(vectorizer_sentiments.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. □ (2pts) Split the dataset into 80% for training and 20% for testing. For this, you can use `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set = 137456\n",
      "Size of testing set = 34364\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Split the dataset\n",
    "training_dataset, testing_dataset = train_test_split(\n",
    "    numpy_dataset, train_size=0.8, test_size=0.2)\n",
    "\n",
    "# Split the feature vector of emotions\n",
    "training_X_emotions, testing_X_emotions = train_test_split(\n",
    "    X_emotions, train_size=0.8, test_size=0.2)\n",
    "\n",
    "# Split the feature vector of sentiments\n",
    "training_X_sentiments, testing_X_sentiments = train_test_split(\n",
    "    X_sentiments, train_size=0.8, test_size=0.2)\n",
    "\n",
    "# Print the size of both datasets\n",
    "print(\"Size of training set =\", training_dataset.shape[0])\n",
    "print(\"Size of testing set =\", testing_dataset.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Train and test the following classifiers, for both the emotion and the sentiment classification, using\n",
    "word frequency as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.1. □ (3pts) **Base-MNB**: a Multinomial Naive Bayes Classifier `(naive_bayes.MultinomialNB.html)`\n",
    "with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'neutral' 'neutral' ... 'optimism' 'neutral' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "# Create the object classifiers for emotions\n",
    "emotions_classifier_mb = MultinomialNB()\n",
    "\n",
    "# Fit the model with training_X as X and columns of training_dataset as y\n",
    "emotions_classifier_mb.fit(X=training_X_emotions,\n",
    "                           y=training_dataset[:, 1])\n",
    "\n",
    "# Make predictions with testing_X as X\n",
    "emotion_prediction_mb = emotions_classifier_mb.predict(X=testing_X_emotions)\n",
    "print(emotion_prediction_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'positive' 'positive' ... 'neutral' 'positive' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Create the object classifiers for sentiments\n",
    "sentiment_classifier_mb = MultinomialNB()\n",
    "\n",
    "# Fit the model with training_X as X and columns of training_dataset as y\n",
    "sentiment_classifier_mb.fit(X=training_X_sentiments,\n",
    "                            y=training_dataset[:, 2])\n",
    "\n",
    "# Make predictions with testing_X as X\n",
    "sentiment_prediction_mb = sentiment_classifier_mb.predict(\n",
    "    X=testing_X_sentiments)\n",
    "print(sentiment_prediction_mb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.4 for Multinomial classification\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Append Emotions results\n",
    "performance_file = open(\"performance\", \"w\")\n",
    "performance_file.write(\n",
    "    \"-----Emotions classification (Multinomial Naive Bayes)-----\\n\")\n",
    "\n",
    "performance_file.write(\n",
    "    f\"Emotions hyperparameters = {emotions_classifier_mb.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(\n",
    "    y_true=testing_dataset[:, 1], y_pred=emotion_prediction_mb)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(\n",
    "    y_true=testing_dataset[:, 1], y_pred=emotion_prediction_mb, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(\n",
    "    f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append Sentiments results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\n",
    "    \"-----Sentiments classification (Multinomial Naive Bayes)-----\\n\")\n",
    "\n",
    "performance_file.write(\n",
    "    f\"Sentiments hyperparameters = {sentiment_classifier_mb.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(\n",
    "    y_true=testing_dataset[:, 1], y_pred=sentiment_prediction_mb)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(\n",
    "    y_true=testing_dataset[:, 1], y_pred=sentiment_prediction_mb, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(\n",
    "    f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.2. □ (3pts) **Base-DT:** a Decision Tree `(tree.DecisionTreeClassifier)` with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['curiosity' 'neutral' 'neutral' ... 'optimism' 'admiration' 'admiration']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Create the object classifiers for emotions\n",
    "emotions_classifier_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model with training_X as X and columns of training_dataset as y\n",
    "emotions_classifier_dt.fit(X=training_X_emotions,\n",
    "                           y=training_dataset[:, 1])\n",
    "\n",
    "# Make predictions with testing_X as X\n",
    "emotion_prediction_dt = emotions_classifier_dt.predict(X=testing_X_emotions)\n",
    "print(emotion_prediction_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive' 'negative' 'positive' ... 'neutral' 'negative' 'negative']\n"
     ]
    }
   ],
   "source": [
    "# Create the object classifiers for sentiments\n",
    "sentiment_classifier_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model with training_X as X and columns of training_dataset as y\n",
    "sentiment_classifier_dt.fit(X=training_X_sentiments,\n",
    "                            y=training_dataset[:, 2])\n",
    "\n",
    "# Make predictions with testing_X as X\n",
    "sentiment_prediction_dt = sentiment_classifier_dt.predict(\n",
    "    X=testing_X_sentiments)\n",
    "print(sentiment_prediction_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.4 for DecisionTree classification\n",
    "\n",
    "\n",
    "# Append Emotions results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\"-----Emotions classification (Decision Tree)-----\\n\")\n",
    "\n",
    "performance_file.write(\n",
    "    f\"Emotions hyperparameters = {emotions_classifier_dt.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(\n",
    "    y_true=testing_dataset[:, 1], y_pred=emotion_prediction_dt)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(\n",
    "    y_true=testing_dataset[:, 1], y_pred=emotion_prediction_dt, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(\n",
    "    f\"----------------------------------------------------------\\n\\n\")\n",
    "performance_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Append Sentiments results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\n",
    "    \"-----Sentiments classification (Multinomial Naive Bayes)-----\\n\")\n",
    "\n",
    "performance_file.write(\n",
    "    f\"Sentiments hyperparameters = {sentiment_classifier_dt.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(\n",
    "    y_true=testing_dataset[:, 1], y_pred=sentiment_prediction_dt)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(\n",
    "    y_true=testing_dataset[:, 1], y_pred=sentiment_prediction_dt, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(\n",
    "    f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.3. □ (3pts) **Base-MLP:** a Multi-Layered Perceptron `(neural network.MLPClassifier)` with the\n",
    "default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.4. □ (3pts) **Top-MNB:** a better performing Multinomial Naive Bayes Classifier found using `GridSearchCV`.\n",
    "The gridsearch will allow you to find the best combination of hyper-parameters, as determined\n",
    "by the evaluation function that you have determined in step 1.3. The only hyper-parameter that\n",
    "you will experiment with is `alphafloat` with values 0.5, 0 and 2 other values of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:591: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neutral' 'neutral' 'neutral' ... 'neutral' 'neutral' 'neutral']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# hyperparameter used in gridsearch\n",
    "hyperparam = {'alpha': [0, 0.5, 1.0, 5.0]}\n",
    "\n",
    "# emotions gridsearch for Top Multinomial Naive Bayes\n",
    "emo_top_mnb_gridsearch = GridSearchCV(emotions_classifier_mb, param_grid=hyperparam)\n",
    "emo_top_mnb_gridsearch.fit(X=training_X_emotions, y=training_dataset[:, 1])\n",
    "emo_prediction_tmb = emo_top_mnb_gridsearch.predict(X=testing_X_emotions)\n",
    "print(emo_prediction_tmb)\n",
    "\n",
    "# sentiments gridsearch for Top Multinomial Naive Bayes\n",
    "sen_top_mnb_gridsearch = GridSearchCV(sentiment_classifier_mb, param_grid=hyperparam)\n",
    "sen_top_mnb_gridsearch.fit(X=training_X_sentiments, y=training_dataset[:, 1])\n",
    "sen_prediction_tmb = sen_top_mnb_gridsearch.predict(X=testing_X_sentiments)\n",
    "print(sen_prediction_tmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.4 for Top Multinomial Naive Bayes classification with GridSearchCV (Emotions)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Append Emotions results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\"-----Emotions classification (Top Multinomial Naive Bayes with GridSearchCV)-----\\n\")\n",
    "\n",
    "performance_file.write(f\"Emotions hyperparamenters = {emo_top_mnb_gridsearch.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(y_true=testing_dataset[:, 1], y_pred=emo_prediction_tmb)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(y_true=testing_dataset[:, 1], y_pred=emo_prediction_tmb, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.4 for Top Multinomial Naive Bayes classification with GridSearchCV (Sentiment)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Append Emotions results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\"-----Sentiment classification (Top Multinomial Naive Bayes with GridSearchCV)-----\\n\")\n",
    "\n",
    "performance_file.write(f\"Sentiment hyperparamenters = {sen_top_mnb_gridsearch.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(y_true=testing_dataset[:, 1], y_pred=sen_prediction_tmb)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(y_true=testing_dataset[:, 1], y_pred=sen_prediction_tmb, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.5. □ (3pts) **Top-DT:** a better performing Decision Tree found using `GridSearchCV.` The hyperparameters\n",
    "that you will experiment with are:\n",
    "  * `criterion:` gini or entropy\n",
    "  * `max depth:` 2 different values of your choice\n",
    "  * `min samples split:` 3 different values of your choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 2.3.6. □ (3pts) **Top-MLP:** a better performing Multi-Layered Perceptron found using GridSearchCV.\n",
    "The hyper-parameters that you will experiment with are:\n",
    "    * `activation:` sigmoid, tanh, relu and identity\n",
    "    * 2 network architectures of your choice: for eg, 2 hidden layers with 30+50 nodes and 3 hidden\n",
    "layers with 10 + 10 + 10\n",
    "    * `solver:` Adam and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:709: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "50 fits failed out of a total of 80.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 385, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 503, in _validate_hyperparameters\n",
      "    raise ValueError(\n",
      "ValueError: The activation 'sigmoid' is not supported. Supported activations are ['identity', 'logistic', 'relu', 'softmax', 'tanh'].\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "30 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 762, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 385, in _fit\n",
      "    self._validate_hyperparameters()\n",
      "  File \"c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 511, in _validate_hyperparameters\n",
      "    raise ValueError(\n",
      "ValueError: The solver Adam is not supported.  Expected one of: sgd, adam, lbfgs\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan 0.32280875\n",
      "        nan 0.32280875        nan 0.32280875        nan 0.32280875\n",
      "        nan 0.32280148        nan 0.32275783]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mateo\\OneDrive\\Desktop\\COMP472\\mini_project\\COMP472_Repository\\MP1.ipynb Cell 33\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m hyperparam \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mactivation\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtanh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39midentity\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mhidden_layer_sizes\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m],\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39msolver\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msgd\u001b[39m\u001b[39m'\u001b[39m]}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m emo_top_mlp_gridsearch \u001b[39m=\u001b[39m GridSearchCV(MLPClassifier(), param_grid\u001b[39m=\u001b[39mhyperparam)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m emo_top_mlp_gridsearch\u001b[39m.\u001b[39;49mfit(X\u001b[39m=\u001b[39;49mtraining_X_emotions, y\u001b[39m=\u001b[39;49mtraining_dataset[:, \u001b[39m1\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m emo_prediction_tmlp \u001b[39m=\u001b[39m emo_top_mlp_gridsearch\u001b[39m.\u001b[39mpredict(X\u001b[39m=\u001b[39mtesting_X_emotions)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mateo/OneDrive/Desktop/COMP472/mini_project/COMP472_Repository/MP1.ipynb#Y122sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(emo_prediction_tmlp)\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:910\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    908\u001b[0m refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m    909\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 910\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    911\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    912\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_\u001b[39m.\u001b[39mfit(X, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:762\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y):\n\u001b[0;32m    746\u001b[0m     \u001b[39m\"\"\"Fit the model to data matrix X and target(s) y.\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \n\u001b[0;32m    748\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[39m        Returns a trained MLP model.\u001b[39;00m\n\u001b[0;32m    761\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 762\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, incremental\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:394\u001b[0m, in \u001b[0;36mBaseMultilayerPerceptron._fit\u001b[1;34m(self, X, y, incremental)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhidden_layer_sizes must be > 0, got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m hidden_layer_sizes\n\u001b[0;32m    389\u001b[0m     )\n\u001b[0;32m    390\u001b[0m first_pass \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoefs_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[0;32m    391\u001b[0m     \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarm_start \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m incremental\n\u001b[0;32m    392\u001b[0m )\n\u001b[1;32m--> 394\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_input(X, y, incremental, reset\u001b[39m=\u001b[39;49mfirst_pass)\n\u001b[0;32m    396\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n\u001b[0;32m    398\u001b[0m \u001b[39m# Ensure y is 2D\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:1159\u001b[0m, in \u001b[0;36mMLPClassifier._validate_input\u001b[1;34m(self, X, y, incremental, reset)\u001b[0m\n\u001b[0;32m   1152\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1153\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`y` has classes not in `self.classes_`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1154\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`self.classes_` has \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39my\u001b[39m\u001b[39m'\u001b[39m\u001b[39m has \u001b[39m\u001b[39m{\u001b[39;00mclasses\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1155\u001b[0m         )\n\u001b[0;32m   1157\u001b[0m \u001b[39m# This downcast to bool is to prevent upcasting when working with\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39m# float32 data\u001b[39;00m\n\u001b[1;32m-> 1159\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_label_binarizer\u001b[39m.\u001b[39;49mtransform(y)\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)\n\u001b[0;32m   1160\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:353\u001b[0m, in \u001b[0;36mLabelBinarizer.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[39mif\u001b[39;00m y_is_multilabel \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my_type_\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    351\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThe object was not fitted with multilabel input.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 353\u001b[0m \u001b[39mreturn\u001b[39;00m label_binarize(\n\u001b[0;32m    354\u001b[0m     y,\n\u001b[0;32m    355\u001b[0m     classes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclasses_,\n\u001b[0;32m    356\u001b[0m     pos_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpos_label,\n\u001b[0;32m    357\u001b[0m     neg_label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mneg_label,\n\u001b[0;32m    358\u001b[0m     sparse_output\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse_output,\n\u001b[0;32m    359\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:539\u001b[0m, in \u001b[0;36mlabel_binarize\u001b[1;34m(y, classes, neg_label, pos_label, sparse_output)\u001b[0m\n\u001b[0;32m    532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    533\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mclasses \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m mismatch with the labels \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m found in the data\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    534\u001b[0m                 classes, unique_labels(y)\n\u001b[0;32m    535\u001b[0m             )\n\u001b[0;32m    536\u001b[0m         )\n\u001b[0;32m    538\u001b[0m \u001b[39mif\u001b[39;00m y_type \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 539\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y)\n\u001b[0;32m    541\u001b[0m     \u001b[39m# pick out the known labels from y\u001b[39;00m\n\u001b[0;32m    542\u001b[0m     y_in_classes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39min1d(y, classes)\n",
      "File \u001b[1;32mc:\\Users\\mateo\\miniconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1144\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, warn)\u001b[0m\n\u001b[0;32m   1142\u001b[0m shape \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mshape(y)\n\u001b[0;32m   1143\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m-> 1144\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mravel(y)\n\u001b[0;32m   1145\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(shape) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m \u001b[39mand\u001b[39;00m shape[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1146\u001b[0m     \u001b[39mif\u001b[39;00m warn:\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mravel\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# hyperparameter used in gridsearch\n",
    "hyperparam = {'activation': ['sigmoid', 'tanh', 'relu', 'identity'],\n",
    "                'hidden_layer_sizes': [2, 3],\n",
    "                'solver': ['Adam', 'sgd']}\n",
    "\n",
    "emo_top_mlp_gridsearch = GridSearchCV(MLPClassifier(), param_grid=hyperparam)\n",
    "emo_top_mlp_gridsearch.fit(X=training_X_emotions, y=training_dataset[:, 1])\n",
    "emo_prediction_tmlp = emo_top_mlp_gridsearch.predict(X=testing_X_emotions)\n",
    "print(emo_prediction_tmlp)\n",
    "\n",
    "\n",
    "sen_top_mlp_gridsearch = GridSearchCV(MLPClassifier(), param_grid=hyperparam)\n",
    "sen_top_mlp_gridsearch.fit(X=training_X_emotions, y=training_dataset[:, 1])\n",
    "sen_prediction_tmlp = sen_top_mlp_gridsearch.predict(X=testing_X_emotions)\n",
    "print(emo_prediction_tmlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.4 for Top Multi-Layered Percentron classification with GridSearchCV (Emotions)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Append Emotions results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\"-----Emotions classification (Top Multi-Layered Percentron with GridSearchCV)-----\\n\")\n",
    "\n",
    "performance_file.write(f\"Emotions hyperparamenters = {emo_top_mlp_gridsearch.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(y_true=testing_dataset[:, 1], y_pred=emo_prediction_tmlp)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(y_true=testing_dataset[:, 1], y_pred=emo_prediction_tmlp, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2.4 for Top Multi-Layered Percentron classification with GridSearchCV (Sentiment)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Append Sentiment results\n",
    "performance_file = open(\"performance\", \"a\")\n",
    "performance_file.write(\"-----Sentiment classification (Top Multi-Layered Percentron with GridSearchCV)-----\\n\")\n",
    "\n",
    "performance_file.write(f\"Sentiment hyperparamenters = {sen_top_mlp_gridsearch.n_features_in_}\\n\")\n",
    "\n",
    "confusion_matrix_output = confusion_matrix(y_true=testing_dataset[:, 1], y_pred=sen_prediction_tmlp)\n",
    "performance_file.write(f\"Confusion Matrix = \\n{confusion_matrix_output}\\n\\n\")\n",
    "\n",
    "class_report = classification_report(y_true=testing_dataset[:, 1], y_pred=sen_prediction_tmlp, zero_division=0)\n",
    "performance_file.write(f\"Classification Report = \\n{class_report}\\n\")\n",
    "performance_file.write(f\"----------------------------------------------------------\\n\\n\")\n",
    "\n",
    "performance_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. □ (5pts) For each of the 6 classifiers above and each of the classification tasks (emotion or sentiment),\n",
    "produce and save the following information in a file called `performance`:\n",
    "* a string clearly describing the model (e.g. the model name + hyper-parameter values) and the\n",
    "classification task (emotion or sentiment)\n",
    "* the confusion matrix – use `metrics.confusion_matrix`\n",
    "* the precision, recall, and F1-measure for each class, and the accuracy, macro-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.5. □ (7.5pts) **Do your own exploration:** Do only one of the following, depending on your own interest:\n",
    "* Use tf-idf instead of word frequencies and redo all substeps of 2.3 above – you can use `TfidfTransformer`\n",
    "for this. Display the results of this experiment.\n",
    "* Remove stop words and redo all substeps of 2.3 above – you can use the parameter of `CountVectorizer`\n",
    "for this. Display the results of this experiment.\n",
    "* Play with `train_test_split` in order have different splits of 80% training, 20% test sets and\n",
    "different sizes of training sets and redo all substeps of 2.3 above. Show and explain how the\n",
    "performance of your models vary depending on the training/test sets are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Embeddings as Features (20pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. □ (0pts) Use `gensim.downloader.load` to load the `word2vec-google-news-300` pretrained embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import downloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. □ (2pts) Use the `tokenizer` from `nltk` to extract words from the Reddit posts. Display the number\n",
    "of tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "# token_phrases = nltk.data.load(list(phrases))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cba032392a64356f8a966ab10381b4d5769feeef7791e19de1420d493f752a7b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
